{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS5110 - Introduction to Data Management and Processing\n",
    "\n",
    "- Instructor: Prof. Mohammed Toutiaee\n",
    "- Semester: Fall 2023\n",
    "- Class Timings: Thursday 14:00 - 17:30 Pacific Time\n",
    "- Class Location: Northeastern University Silicon Valley, Room 303"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code for the top 5 models which were built by students for HW4. The HW4 assignment is identical to a Kaggle competition and the leaderboard has been announced in the class. Please readon for the code and the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Financial institutions that lend to consumers rely on models to help decide on who to approve or decline for credit (for lending products such as credit cards, automobile loans, or home loans). In this project, your task is to develop models that review credit card applications to determine which ones should be approved. You are given historical data on response (binary default indicator) and 20 predictor variables from credit card accounts for a hypothetical bank XYZ, a regional bank in the Bay area. There are three datasets available: a [training](https://raw.githubusercontent.com/mh2t/DS5110/main/Homework/HW4-Train.csv) dataset with 20,000 accounts; a [validation](https://raw.githubusercontent.com/mh2t/DS5110/main/Homework/HW4-Validation.csv) dataset with 3,000 accounts, and a **hidden** test dataset with 5,000 accounts. Information about the variables is given in the [Appendix](https://github.com/mh2t/DS5110/blob/main/Homework/HW4-appx.pdf).\n",
    "\n",
    "You are asked to do the following and also address specific questions below:\n",
    "\n",
    "* **(10 points)** Do any necessary data pre-processing in preparation for modeling.\n",
    "* **(20 points)** Develop and fit a logistic regression (LR) model, assess its performance, and interpret the results.\n",
    "* **(20 points)** Develop an additional model based on a machine learning (ML) algorithm selected from one of the following: Random Forest, Gradient Boosting (XGBoost or another implementation), or Feedforward Neural Network; assess its performance, and make sure to explain why you chose this particular algorithm.\n",
    "* **(10 points)** Compare the results from the ML algorithm with those from logistic regression model and discuss their advantages and disadvantages; select one of these models for credit approval; and describe the reasons for your selection.\n",
    "* **(5 points)** Describe what performance metrics you chose to evalaute your proposed models and why.\n",
    "* **(10 points)** Describe how you would use it to make decisions on future credit card applications.\n",
    "* **(5 points)** Do customers who already have an account with the financial institution receive any favorable treatment in your model? Support your answer with appropriate analysis.\n",
    "* **(20 points)** 2-page report.\n",
    "* You can use any libraries for this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "Please submit the following:\n",
    "\n",
    "1. A report (doc file) that describes all important steps in your data analysis,\n",
    "model development, comparison of the models, and answer to the specific questions in addition to justification for your final model selection. The body of the report should be no more than 2 pages in length (font size 11 and spacing 1.2).\n",
    "2. The codes you used for the analysis should have brief but adequate annotations so that we can run it. Using a format of **IPYNB** is mandatory. Clearly indicate the software packages and versions (if appropriate) that you used for the analysis.\n",
    "3. You are allowed to review textbooks, published papers, websites, and other open literature in preparing for this homework. Note, however, that the material you submit in your report must be based on your own analysis and writing. If you relied on published scholarly work and open-source software for your analysis and findings (beyond what is generally known), you should provide references at the end of the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Model Bonus\n",
    "\n",
    "If the evaluation metric of your chosen model achieve the **highest** rank among all submissions, you will be awarded an additional **10 bonus points**. This bonus will be directly applied to your homework 4 score. It's important to note that the performance of your best model will be assessed using a hidden test set, ensuring a fair and unbiased evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric\n",
    "\n",
    "After the students submitted their models, thier models were run on a hidden dataset and were score using the following metric:  \n",
    "\n",
    "$$ F_2 = \\frac{5 \\times Precision \\times Recall}{4 \\times Precision + Recall} $$\n",
    "The cells below depict the code for the top 5 models and are ordered by their rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global and common imports\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy import stats  # import norm, zscore, chi2_contingency\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    make_scorer,\n",
    "    roc_curve,\n",
    "    fbeta_score,\n",
    "    auc,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    RepeatedStratifiedKFold,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    RandomizedSearchCV,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import plot_importance, XGBClassifier, DMatrix, train, plot_importance\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "\n",
    "- F2-Score: 0.5693\n",
    "- Model Name: XGBoost\n",
    "- Model Type: Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages needed for the Modeling.\n",
    "\n",
    "train_data = pd.read_csv(\"location of the train data\")\n",
    "\n",
    "columns_to_drop = [\"Default_ind\", \"States\"]\n",
    "\n",
    "\n",
    "def normal_data():\n",
    "    X_train = train_data.drop(labels=columns_to_drop, axis=1)\n",
    "    y_train = train_data.Default_ind\n",
    "    scaler = StandardScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "\n",
    "    v_data = pd.read_csv(\n",
    "        \"\"\n",
    "    )\n",
    "    v_data.dropna(inplace=True)\n",
    "    v_data[\"combined\"] = v_data[\"avg_card_debt\"] * v_data[\"uti_card\"]\n",
    "\n",
    "    # Dropping the States and auto_open_36_month_num column due to it insignificance we learnt from chi-square test\n",
    "    X_test = v_data.drop(labels=columns_to_drop, axis=1)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    y_test = v_data.Default_ind\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = normal_data()\n",
    "\n",
    "# Train an XGBoost classifier\n",
    "xgb_model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=111,\n",
    "    scale_pos_weight=5,\n",
    "    max_depth=2,\n",
    "    eval_metric=\"logloss\",\n",
    "    enable_categorical=\"missing\",\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "f2_score = fbeta_score(y_test, y_pred, beta=2)\n",
    "\n",
    "print(f'F2 Score: {f2_score}')\n",
    "\n",
    "\n",
    "# Display the results\n",
    "# Calculate ROC curve\n",
    "print(\"ROC Curve:\")\n",
    "print(roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "cm_display = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=conf_matrix, display_labels=[False, True]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "cm_display.plot(ax=ax)\n",
    "plot_importance(xgb_model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "\n",
    "- F2-Score: 0.5671\n",
    "- Model Name: GradientBoosting\n",
    "- Model Type: Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = df.select_dtypes(include=\"number\").columns\n",
    "categorical_cols = df.select_dtypes(exclude=\"number\").columns\n",
    "\n",
    "numeric_imputer = SimpleImputer(strategy=\"mean\")\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# Impute missing values in numeric columns with the mean\n",
    "df[numeric_cols] = numeric_imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Impute missing values in categorical columns with the most frequent value\n",
    "df[categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_no_missing = df.dropna()\n",
    "\n",
    "# Drop columns with any missing values\n",
    "df_no_missing_cols = df.dropna(axis=1)\n",
    "\n",
    "# Assuming df is your DataFrame with multiple categorical columns\n",
    "\n",
    "# Create a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", MinMaxScaler(), numeric_cols),\n",
    "        (\"cat\", OneHotEncoder(), categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the transformation\n",
    "scaled_features = preprocessor.fit_transform(df)\n",
    "\n",
    "# Create a new DataFrame with scaled features\n",
    "columns = list(numeric_cols) + list(\n",
    "    preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_cols)\n",
    ")\n",
    "\n",
    "df_scaled = pd.DataFrame(scaled_features, columns=columns)\n",
    "\n",
    "\n",
    "data = df\n",
    "a = data.dtypes\n",
    "a = pd.DataFrame(a)\n",
    "a = a.reset_index()\n",
    "a = a[a[0] == \"object\"]\n",
    "a\n",
    "\n",
    "\n",
    "def classification(data, name):\n",
    "    a = list(data[name].unique())\n",
    "    b = []\n",
    "    c = list(data[name])\n",
    "    for i in c:\n",
    "        judge = False\n",
    "        for j in range(len(a)):\n",
    "            if a[j] == i:\n",
    "                b.append(j)\n",
    "                judge = True\n",
    "        if judge == False:\n",
    "            b.append(len(a))\n",
    "\n",
    "    data = data.drop(columns=[name])\n",
    "    data.insert(loc=0, column=name, value=b)\n",
    "    return data\n",
    "\n",
    "\n",
    "s = list(a[\"index\"])\n",
    "for i in s:\n",
    "    data = classification(data, i)\n",
    "data\n",
    "\n",
    "\n",
    "\n",
    "data1 = pd.get_dummies(data, dtype=int)\n",
    "X_train = data1.drop([\"Default_ind\"], axis=1)\n",
    "y_train = data1[\"Default_ind\"]\n",
    "\n",
    "# Undersample process\n",
    "undersample = RandomUnderSampler(sampling_strategy=1 / 1)\n",
    "X_train, y_train = undersample.fit_resample(X_train, y_train)\n",
    "print(\"After undersampling: \", Counter(y_train))\n",
    "\n",
    "df = pd.read_csv()\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = df.select_dtypes(include=\"number\").columns\n",
    "categorical_cols = df.select_dtypes(exclude=\"number\").columns\n",
    "\n",
    "# Impute missing values in numeric columns with the mean\n",
    "numeric_imputer = SimpleImputer(strategy=\"mean\")\n",
    "df[numeric_cols] = numeric_imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Impute missing values in categorical columns with the most frequent value\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "df[categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])\n",
    "# Drop rows with any missing values\n",
    "df_no_missing = df.dropna()\n",
    "\n",
    "# Drop columns with any missing values\n",
    "df_no_missing_cols = df.dropna(axis=1)\n",
    "# Assuming df is your DataFrame with multiple categorical columns\n",
    "# Create a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", MinMaxScaler(), numeric_cols),\n",
    "        (\"cat\", OneHotEncoder(), categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the transformation\n",
    "scaled_features = preprocessor.fit_transform(df)\n",
    "\n",
    "# Create a new DataFrame with scaled features\n",
    "columns = list(numeric_cols) + list(\n",
    "    preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_cols)\n",
    ")\n",
    "df_scaled = pd.DataFrame(scaled_features, columns=columns)\n",
    "\n",
    "print(\"\\nScaled DataFrame:\")\n",
    "print(df_scaled)\n",
    "\n",
    "\n",
    "data = df\n",
    "a = data.dtypes\n",
    "a = pd.DataFrame(a)\n",
    "a = a.reset_index()\n",
    "a = a[a[0] == \"object\"]\n",
    "a\n",
    "\n",
    "\n",
    "def classification(data, name):\n",
    "    a = list(data[name].unique())\n",
    "    b = []\n",
    "    c = list(data[name])\n",
    "    for i in c:\n",
    "        judge = False\n",
    "        for j in range(len(a)):\n",
    "            if a[j] == i:\n",
    "                b.append(j)\n",
    "                judge = True\n",
    "        if judge == False:\n",
    "            b.append(len(a))\n",
    "\n",
    "    data = data.drop(columns=[name])\n",
    "    data.insert(loc=0, column=name, value=b)\n",
    "    return data\n",
    "\n",
    "\n",
    "s = list(a[\"index\"])\n",
    "for i in s:\n",
    "    data = classification(data, i)\n",
    "data\n",
    "\n",
    "\n",
    "data2 = pd.get_dummies(data, dtype=int)\n",
    "data2\n",
    "\n",
    "# Create a Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingClassifier(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# model predict\n",
    "x_val = data2.drop([\"Default_ind\"], axis=1)\n",
    "y_val = data2[\"Default_ind\"]\n",
    "\n",
    "resu = gb_regressor.predict(x_val)\n",
    "fpr1, tpr1, threshold = roc_curve(y_val, resu)\n",
    "\n",
    "# Calculate ROC and AUC\n",
    "fpr, tpr, threshold = roc_curve(y_val, resu)\n",
    "rocauc = auc(fpr1, tpr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "\n",
    "- F2-Score: 0.5611\n",
    "- Model Name: XGBoost\n",
    "- Model Type: Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I select XGBoost\n",
    "# RandomizedSearchCV for text\n",
    "\n",
    "\n",
    "training = pd.read_csv()\n",
    "validation = pd.read_csv()\n",
    "\n",
    "# Delete NA\n",
    "training.dropna(inplace=True)\n",
    "training\n",
    "\n",
    "# instead States name into number\n",
    "training[\"States\"] = training[\"States\"].replace(\n",
    "    {\n",
    "        \"NC\": 53740,\n",
    "        \"AL\": 47466,\n",
    "        \"FL\": 59521,\n",
    "        \"GA\": 53194,\n",
    "        \"LA\": 51571,\n",
    "        \"MS\": 43654,\n",
    "        \"SC\": 50341,\n",
    "    }\n",
    ")\n",
    "\n",
    "# def remove_outliers(df):\n",
    "#     Q1 = df.quantile(0.25)\n",
    "#     Q3 = df.quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     df_out = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "#     return df_out\n",
    "# training = remove_outliers(training)\n",
    "# training\n",
    "validation.dropna(inplace=True)\n",
    "\n",
    "validation[\"States\"] = validation[\"States\"].replace(\n",
    "    {\n",
    "        \"NC\": 53740,\n",
    "        \"AL\": 47466,\n",
    "        \"FL\": 59521,\n",
    "        \"GA\": 53194,\n",
    "        \"LA\": 51571,\n",
    "        \"MS\": 43654,\n",
    "        \"SC\": 50341,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# we need to choose coloum which is Gaussian Distribution to do standalization\n",
    "columns_to_standardize = [\n",
    "    0,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    13,\n",
    "    14,\n",
    "    15,\n",
    "    16,\n",
    "    18,\n",
    "]  # choose column by using index\n",
    "training_standardized = scaler.fit_transform(training.iloc[:, columns_to_standardize])\n",
    "\n",
    "# put standardization data into df\n",
    "training.iloc[:, columns_to_standardize] = training_standardized\n",
    "\n",
    "training\n",
    "\n",
    "# we will do normalization\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# normalization\n",
    "columns_to_normalize = [\n",
    "    1,\n",
    "    5,\n",
    "    6,\n",
    "    7,\n",
    "    8,\n",
    "    9,\n",
    "    10,\n",
    "    11,\n",
    "    12,\n",
    "    17,\n",
    "    19,\n",
    "    20,\n",
    "]  # choose column that we need normalization\n",
    "training_normalized = min_max_scaler.fit_transform(\n",
    "    training.iloc[:, columns_to_normalize]\n",
    ")\n",
    "\n",
    "# put back to the position\n",
    "training.iloc[:, columns_to_normalize] = training_normalized\n",
    "\n",
    "training\n",
    "\n",
    "# we need to choose coloum which is Gaussian Distribution to do standalization\n",
    "columns_to_standardize = [\n",
    "    0,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    13,\n",
    "    14,\n",
    "    15,\n",
    "    16,\n",
    "    18,\n",
    "]  # choose column by using index\n",
    "validation_standardized = scaler.fit_transform(\n",
    "    validation.iloc[:, columns_to_standardize]\n",
    ")\n",
    "\n",
    "# put standardization data into df\n",
    "validation.iloc[:, columns_to_standardize] = validation_standardized\n",
    "\n",
    "validation\n",
    "\n",
    "# we will do normalization\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# nomalization for test\n",
    "columns_to_normalize = [\n",
    "    1,\n",
    "    5,\n",
    "    6,\n",
    "    7,\n",
    "    8,\n",
    "    9,\n",
    "    10,\n",
    "    11,\n",
    "    12,\n",
    "    17,\n",
    "    19,\n",
    "    20,\n",
    "]  # choose column that we need normalization\n",
    "validation_normalized = min_max_scaler.fit_transform(\n",
    "    validation.iloc[:, columns_to_normalize]\n",
    ")\n",
    "\n",
    "validation.iloc[:, columns_to_normalize] = validation_normalized\n",
    "\n",
    "validation\n",
    "\n",
    "X_train = training.drop(\"Default_ind\", axis=1)\n",
    "y_train = training[\"Default_ind\"]\n",
    "\n",
    "X_test = validation.drop(\"Default_ind\", axis=1)\n",
    "y_test = validation[\"Default_ind\"]\n",
    "\n",
    "# set up XGBoost\n",
    "xgb_model = XGBClassifier(\n",
    "    objective=\"binary:logistic\", use_label_encoder=False, eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "# set up the parameter\n",
    "param_dist = {\n",
    "    \"max_depth\": [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1, 0.2, 0.3],\n",
    "    \"subsample\": [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"gamma\": [0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"reg_lambda\": [0, 0.5, 1, 1.5, 2, 3],\n",
    "    \"reg_alpha\": [0, 0.1, 0.5, 1],\n",
    "    \"min_child_weight\": [1, 2, 3, 4, 5, 6],\n",
    "    \"scale_pos_weight\": [1, 2, 3, 4],\n",
    "}\n",
    "\n",
    "# set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    cv=5,  # more cv more stable cross validation result\n",
    "    verbose=3,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# random search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# print function\n",
    "print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "print(f\"Best AUC found: {random_search.best_score_}\")\n",
    "\n",
    "# use the best model to predict\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# calculate AUC-ROC\n",
    "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC-ROC: {auc_roc}\")\n",
    "\n",
    "# evaluate performance of the best model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the best model: {accuracy * 100.0}%\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Do model predictions and inference here\n",
    "model = best_model\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_true = y_test\n",
    "\n",
    "# Calculate metrics here\n",
    "f2_score = metrics.fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "print(f\"F2 Score: {f2_score}\")\n",
    "\n",
    "feature_importance = pd.DataFrame(\n",
    "    best_model.feature_importances_, index=X_train.columns, columns=[\"Importance\"]\n",
    ")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4\n",
    "\n",
    "- F2-Score: 0.5610\n",
    "- Model Name: XGBoost\n",
    "- Model Type: Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_preprocessing(data, y=\"Default_ind\"):\n",
    "    if \"States\" in data.columns:\n",
    "        data = pd.get_dummies(data, columns=[\"States\"], drop_first=True)\n",
    "\n",
    "    data = data.dropna(axis=0)\n",
    "\n",
    "    features = data.drop(y, axis=1)\n",
    "    target = data[y]\n",
    "\n",
    "    return features, target\n",
    "\n",
    "\n",
    "X_train, y_train = simple_preprocessing(pd.read_csv())\n",
    "X_valid, y_valid = simple_preprocessing(pd.read_csv())\n",
    "\n",
    "bmodel = LogisticRegression()\n",
    "bmodel.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bmodel.predict(X_valid)\n",
    "\n",
    "print(\"Baseline Model : LogisticRegression\")\n",
    "print(\"Accuracy:\", accuracy_score(y_valid, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_valid, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(y_valid, y_pred), display_labels=[0, 1]\n",
    ")\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "\n",
    "# Test baseline model here\n",
    "x = \"\"\n",
    "if x == \"\":\n",
    "    x = \"\"\n",
    "test = pd.read_csv(x)\n",
    "X_test, y_test = simple_preprocessing(test)\n",
    "y_pred = bmodel.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "if hasattr(bmodel, \"decision_function\"):\n",
    "    y_score = bmodel.decision_function(X_test)\n",
    "else:\n",
    "    y_score = bmodel.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"AUROC:\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Create a contingency table\n",
    "contingency_table = pd.crosstab(train[\"States\"], train[\"Default_ind\"])\n",
    "\n",
    "# Perform the chi-square test to check cat-cat variables\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Chi-square statistic: {chi2}\")\n",
    "print(f\"P-value: {p}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "print(\"Expected frequencies table:\")\n",
    "print(expected)\n",
    "\n",
    "\n",
    "def correlation_test(df, thresh=0.8):\n",
    "    columns = df.columns\n",
    "    a = []\n",
    "    for col1 in columns:\n",
    "        for col2 in columns:\n",
    "            if col1 < col2:\n",
    "                cor = df[col1].corr(df[col2])\n",
    "                if cor > thresh or cor < -thresh:\n",
    "                    # print(col1, col2, cor)\n",
    "                    a.append(col1)\n",
    "    return a\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_class,\n",
    "        trainset,\n",
    "        testset,\n",
    "        y=\"Default_ind\",\n",
    "        smote=False,\n",
    "        thresh=0.6,\n",
    "        scale=False,\n",
    "    ):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = model_class\n",
    "        self.y = y\n",
    "        self.scale = scale\n",
    "\n",
    "        X_train, y_train = self.simple_preprocessing(trainset)\n",
    "        self.droppable_columns = correlation_test(X_train, thresh)\n",
    "        if smote == True:\n",
    "            X_train, y_train = self.resample(X_train, y_train)\n",
    "\n",
    "        self.train(X_train, y_train)\n",
    "        self.test(testset)\n",
    "\n",
    "    def resample(self, X, Y):\n",
    "        sm = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = sm.fit_resample(X, Y)\n",
    "        return X_resampled, y_resampled\n",
    "\n",
    "    def simple_preprocessing(self, df):\n",
    "        if self.scale:\n",
    "            df.dropna(axis=0, inplace=True)\n",
    "\n",
    "        if \"States\" in df.columns:\n",
    "            df.drop(\"States\", axis=1, inplace=True)\n",
    "\n",
    "        X_train = df.drop(self.y, axis=1)\n",
    "        y_train = df[self.y]\n",
    "        return X_train, y_train\n",
    "\n",
    "    def preprocess(self, X):\n",
    "        X.drop(labels=self.droppable_columns, axis=1, inplace=True)\n",
    "        return X\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        X_train = self.preprocess(X_train)\n",
    "        if self.scale:\n",
    "            X_train = self.scaler.fit_transform(X_train)\n",
    "        if issubclass(self.model.__class__, xgb.XGBClassifier):\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            self.model = xgb.train(self.model.get_params(), dtrain, num_boost_round=132)\n",
    "        else:\n",
    "            self.model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    def test(self, testset):\n",
    "        X_test, y_test = self.simple_preprocessing(testset)\n",
    "        X_test = self.preprocess(X_test)\n",
    "        if self.scale:\n",
    "            X_test = self.scaler.transform(X_test)\n",
    "        if issubclass(self.model.__class__, xgb.Booster):\n",
    "            dtest = xgb.DMatrix(X_test)\n",
    "            y_pred = self.model.predict(dtest)\n",
    "        else:\n",
    "            y_pred = self.model.predict(X_test)\n",
    "\n",
    "        binary_predictions = [1 if p > 0.5 else 0 for p in y_pred]\n",
    "\n",
    "        self.accuracy = accuracy_score(y_test, binary_predictions)\n",
    "        self.report = classification_report(y_test, binary_predictions)\n",
    "        self.c_matrix = confusion_matrix(y_test, binary_predictions)\n",
    "\n",
    "        if issubclass(self.model.__class__, xgb.Booster):\n",
    "            # For XGBoost, use the predicted probabilities directly\n",
    "            self.fpr, self.tpr, _ = roc_curve(y_test, y_pred)\n",
    "            self.auroc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "            self.f2_score = fbeta_score(y_test, binary_predictions, beta=2)\n",
    "        else:\n",
    "            # For other models, use the decision function or predicted probabilities\n",
    "            if hasattr(self.model, \"decision_function\"):\n",
    "                y_score = self.model.decision_function(X_test)\n",
    "            else:\n",
    "                y_score = self.model.predict_proba(X_test)[:, 1]\n",
    "            self.fpr, self.tpr, _ = roc_curve(y_test, y_score)\n",
    "            self.auroc = roc_auc_score(y_test, y_score)\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def all_stats(self):\n",
    "        print(f\"{self.model.__class__.__name__} Model\")\n",
    "        print(\"Accuracy:\", self.accuracy)\n",
    "        print(\"Classification Report:\\n\", self.report)\n",
    "        print(\"AUROC:\", self.auroc)\n",
    "        print(\"Confusion Matrix:\")\n",
    "\n",
    "        # Create subplots\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        # Plot AUROC\n",
    "        axs[0].plot(\n",
    "            self.fpr,\n",
    "            self.tpr,\n",
    "            color=\"darkorange\",\n",
    "            lw=2,\n",
    "            label=\"ROC curve (area = {:.2f})\".format(self.auroc),\n",
    "        )\n",
    "        axs[0].plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "        axs[0].set_xlabel(\"False Positive Rate\")\n",
    "        axs[0].set_ylabel(\"True Positive Rate\")\n",
    "        axs[0].set_title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "        axs[0].legend(loc=\"lower right\")\n",
    "\n",
    "        # Plot Confusion Matrix\n",
    "        if issubclass(self.model.__class__, xgb.Booster):\n",
    "            classes = [0, 1]\n",
    "            disp = ConfusionMatrixDisplay(\n",
    "                confusion_matrix=self.c_matrix, display_labels=classes\n",
    "            )\n",
    "        else:\n",
    "            disp = ConfusionMatrixDisplay(\n",
    "                confusion_matrix=self.c_matrix, display_labels=self.model.classes_\n",
    "            )\n",
    "        disp.plot(ax=axs[1], cmap=\"Blues\", values_format=\"d\")\n",
    "        axs[1].set_title(\"Confusion Matrix\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def res(self):\n",
    "        if issubclass(self.model.__class__, xgb.Booster):\n",
    "            print(\"F2 Score:\", self.f2_score)\n",
    "\n",
    "        print(\"Accuracy:\", self.accuracy)\n",
    "        print(\"AuROC\", self.auroc)\n",
    "\n",
    "\n",
    "train = pd.read_csv(\n",
    "    \"\"\n",
    ")\n",
    "test = pd.read_csv(\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "xgb_model = Model(\n",
    "    xgb.XGBClassifier(\n",
    "        max_depth=3, eta=0.2, objective=\"binary:logistic\", scale_pos_weight=5\n",
    "    ),\n",
    "    trainset=train,\n",
    "    testset=test,\n",
    "    y=\"Default_ind\",\n",
    "    thresh=0.9,\n",
    ")\n",
    "xgb_model.all_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5\n",
    "\n",
    "- F2-Score: 0.5579\n",
    "- Model Name: XGBoost\n",
    "- Model Type: Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_csv()\n",
    "validation_df = pd.read_csv()\n",
    "\n",
    "imputer_uti = SimpleImputer(strategy=\"mean\")\n",
    "imputer_income = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "# Imputation for 'uti_card_50plus_pct'\n",
    "training_df[\"uti_card_50plus_pct\"] = imputer_uti.fit_transform(\n",
    "    training_df[[\"uti_card_50plus_pct\"]]\n",
    ")\n",
    "validation_df[\"uti_card_50plus_pct\"] = imputer_uti.transform(\n",
    "    validation_df[[\"uti_card_50plus_pct\"]]\n",
    ")\n",
    "\n",
    "# Imputation for 'rep_income'\n",
    "training_df[\"rep_income\"] = imputer_income.fit_transform(training_df[[\"rep_income\"]])\n",
    "validation_df[\"rep_income\"] = imputer_income.transform(validation_df[[\"rep_income\"]])\n",
    "\n",
    "X_train_original = training_df.drop([\"Default_ind\", \"States\"], axis=1)\n",
    "y_train_original = training_df[\"Default_ind\"]\n",
    "X_validation_original = validation_df.drop([\"Default_ind\", \"States\"], axis=1)\n",
    "y_validation_original = validation_df[\"Default_ind\"]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# standardizing the data\n",
    "X_train_original_scaled = scaler.fit_transform(X_train_original)\n",
    "X_validation_original_scaled = scaler.transform(X_validation_original)\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=123,\n",
    "    scale_pos_weight=5,\n",
    "    max_depth=2,\n",
    "    eval_metric=\"logloss\",\n",
    "    enable_categorical=\"missing\",\n",
    ")\n",
    "xgb_model.fit(X_train_original_scaled, y_train_original)\n",
    "\n",
    "# Predicting on the validation set\n",
    "y_pred_xgb = xgb_model.predict(X_validation_original_scaled)\n",
    "\n",
    "# Assessing the model's performance\n",
    "classification_rep_xgb = classification_report(y_validation_original, y_pred_xgb)\n",
    "confusion_mat = confusion_matrix(y_validation_original, y_pred_xgb)\n",
    "print(\"Classification Report for XG-Boost Model:\\n\")\n",
    "print(classification_rep_xgb)\n",
    "\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_validation_original_scaled)[:, 1]\n",
    "\n",
    "\n",
    "fpr_xg, tpr_xg, thresholds_xg = roc_curve(y_validation_original, y_pred_proba_xgb)\n",
    "auc_xg = auc(fpr_xg, tpr_xg)\n",
    "\n",
    "print(\"AUC: \", auc_xg)\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Do model predictions and inference here\n",
    "model = xgb_model\n",
    "\n",
    "y_pred = model.predict(X_validation_original_scaled)\n",
    "y_true = y_validation_original\n",
    "\n",
    "# Calculate metrics here\n",
    "f2_score = metrics.fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "print(f2_score)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_mat, display_labels=[False, True]\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "cm_display.plot(ax=ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "northeastern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
